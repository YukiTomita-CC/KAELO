"use strict";(self.webpackChunkkaelo=self.webpackChunkkaelo||[]).push([[5254],{4335:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>d,contentTitle:()=>a,default:()=>_,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var t=n(4848),i=n(8453);const o={sidebar_position:1},a="transformers\u3067\u306e\u4e8b\u524d\u5b66\u7fd2",s={id:"Pre-Training/pretraining_using_transformers",title:"transformers\u3067\u306e\u4e8b\u524d\u5b66\u7fd2",description:"Overview",source:"@site/docs/Pre-Training/pretraining_using_transformers.md",sourceDirName:"Pre-Training",slug:"/Pre-Training/pretraining_using_transformers",permalink:"/KAELO/docs/Pre-Training/pretraining_using_transformers",draft:!1,unlisted:!1,editUrl:"https://github.com/YukiTomita-CC/KAELO/tree/main/docs/Pre-Training/pretraining_using_transformers.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Pre-Training",permalink:"/KAELO/docs/category/pre-training"},next:{title:"Merge",permalink:"/KAELO/docs/category/merge"}},d={},c=[{value:"Overview",id:"overview",level:2},{value:"Install",id:"install",level:2}];function l(e){const r={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"transformers\u3067\u306e\u4e8b\u524d\u5b66\u7fd2",children:"transformers\u3067\u306e\u4e8b\u524d\u5b66\u7fd2"})}),"\n",(0,t.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(r.p,{children:"\u6700\u3082\u57fa\u672c\u7684\u306a\u65b9\u6cd5\u3068\u3057\u3066\u3001transformers\u3092\u4f7f\u3063\u305f\u4e8b\u524d\u5b66\u7fd2\u3092\u8a66\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002"}),"\n",(0,t.jsx)(r.h2,{id:"install",children:"Install"}),"\n",(0,t.jsx)(r.p,{children:"\u4ee5\u4e0b\u306f\u6700\u5c0f\u9650\u306e\u30b5\u30f3\u30d7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u3059\u3002"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\r\nfrom transformers import Trainer, TrainingArguments\r\n\r\nconfig = AutoConfig.from_pretrained(\r\n    "gpt2",\r\n    vocab_size=len(tokenizer),\r\n    n_ctx=context_length,\r\n    bos_token_id=tokenizer.bos_token_id,\r\n    eos_token_id=tokenizer.eos_token_id,\r\n)\r\n\r\nmodel = GPT2LMHeadModel(config)\r\ntokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")\r\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\r\n\r\nargs = TrainingArguments(\r\n    output_dir="codeparrot-ds",\r\n    per_device_train_batch_size=32,\r\n    per_device_eval_batch_size=32,\r\n    evaluation_strategy="steps",\r\n    eval_steps=5_000,\r\n    logging_steps=5_000,\r\n    gradient_accumulation_steps=8,\r\n    num_train_epochs=1,\r\n    weight_decay=0.1,\r\n    warmup_steps=1_000,\r\n    lr_scheduler_type="cosine",\r\n    learning_rate=5e-4,\r\n    save_steps=5_000,\r\n    fp16=True,\r\n    push_to_hub=True,\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    args=args,\r\n    data_collator=data_collator,\r\n    train_dataset=tokenized_datasets["train"],\r\n    eval_dataset=tokenized_datasets["valid"],\r\n)\r\n\r\ntrainer.train()\n'})})]})}function _(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>a,x:()=>s});var t=n(6540);const i={},o=t.createContext(i);function a(e){const r=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function s(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:r},e.children)}}}]);
---
sidebar_position: 2
---

# ローカルLLM with llama.cpp
では、llama.cppを使ってローカルLLMを試していきましょう。

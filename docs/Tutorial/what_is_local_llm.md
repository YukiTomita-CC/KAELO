---
sidebar_position: 1
---

# ローカルLLMって？
まずは、現状のローカルLLMコミュニティの概要をざっくりと認識することから始めます。

## ローカルLLMと商用LLM(ChatGPT、Claude、Gemini...)の違い
技術的な部分が公開されていないため推測になりますが、根本的な仕組みは同じものです。\
多くの場合、LLMはテキストを入力としてテキストや埋め込み表現を出力する機械学習モデルです。

最も異なる部分として、商用LLMはそれを利用するために各社の提供するWebAPIを利用する必要がありますが、ローカルLLMはその名前の通り手元のデバイスのみで動かすことができます。

誤解を恐れずに言えば
- 個人情報やNSFWな内容、機密情報に検閲がかからない
- モデルの深いカスタマイズが可能
- どれだけ使用しても従量課金が発生しない

ということが主なメリットとして挙げられるかと思います。

一方でデメリットもあり
- 商用LLMに比べて汎用的な性能は低い
- カスタマイズには専門的な知識を必要とする場合がある
- ミドルクラス以上のゲーミングPCレベルのハードが要求される

といったものがあるかと思います。

従って、ローカルLLMと商用LLMは競合するものではなく、それぞれの目的別に共存するものであると考えることもできます。

## 主要なモデル、ハード要件
ローカルLLMのコミュニティでは非常に多くのモデルが企業、大学、個人等から公開されています。\
基本的にモデルは[Hugging Face](https://huggingface.co/)というプラットフォームで公開されており、現状では多くのローカルLLMに関するライブラリがここからのモデルのロードを前提としています。

多くのモデルが公開されていることは良いことですが、それによって別の問題が発生しています。

それは **「どのモデルを使ったらいいの？」** という問題です。

この問題については、使用する人が何を求めているか、どんな環境(ハード)を所有しているかで回答が大きく異なってきますので、ここで「このモデルを使えばいい」や「このモデルが使える」ということを示すことはできませんが、そのヒントになるかもしれない情報はお伝え出来ます。

### モデルの探し方
現在、LLMの多くはTransformerというアーキテクチャをベースにしています。

そして、Transformerはその構成によって以下の3つに大別されます。
1. Encoder-Decoder
2. Encoder-Only
3. Decoder-Only

この中でローカルLLMコミュニティにおいて最も勢いのあるものが3のDecoder-Onlyのモデルです。\
これはOpenAIのChatGPTの元となったGPT-3と同様の構成であり、その基本的な原理は入力されたテキストの次の単語(正確にはトークン)を予測するというものですが、その汎用性から様々なモデルが登場しています。

Decoder-Onlyのモデルが得意とするのは「生成」です。文章を作成したり、自然な会話を作ることができます。ただ、その「次の単語を予測する」という性質上、分類や翻訳にも使うこともできます。

では、Decoder-Onlyのモデルはどのように探すのがいいのでしょうか。\
主要な汎用モデルを探す場合は以下のリーダーボードを参照することをお勧めします。
- [Chatbot Arena](https://lmarena.ai/) (LeaderboardタブのCategoryをJapaneseにする)
- [Nejumi LLMリーダーボード3](https://wandb.ai/wandb-japan/llm-leaderboard3/reports/Nejumi-LLM-3--Vmlldzo3OTg2NjM2)

### ハード要件
しかし、リーダーボードで優れたモデルを見つけられたとしても、それが自身のデバイスで動くかどうかは分かりません。

LLM(Large Language Model)は大規模言語モデルとも訳され、その名の通り、巨大なパラメータを持ちます。\
そしてそれを使用するためにはデバイスのメモリ(基本的にはGPUのVRAM)にそのパラメータを全て載せる必要があります。

多くのモデルが、そのモデル名にパラメータ数を明記しています。例えば`elyza/Llama-3-ELYZA-JP-8B`というモデルは`8B`、つまり8 billionのパラメータを持つモデルです。

そして、16bit/パラメータであるので、`8B`モデルを使おうとすると
$$
16 bit * 8,000,000,000 param = 16 GB
$$
となり、これに他の要素として数GB分が必要となるため、`8B`モデルを使うには20GB近くのメモリが必要になります。

これに該当するNVIDIAのコンシューマー向けGPUはRTX3090かRTX4090のどちらかで、いずれもGPU単体で数十万円します。\
上記のリーダーボードの上位を席巻している`70B`などは同様の計算で140GB近くメモリが必要になり、これはA100などの超ハイエンドGPU(数百万円/台)が数台必要です。

では、ほとんどのモデルは動かすことはできないのでしょうか？

先ほど、16bit/パラメータと仮定しましたが、現在のトレンドではモデルを16bitで使うことは少なく、量子化という手法を使って8bit、4bit、果ては1bitまで削減しながらハード要件とモデルの性能のトレードオフを探りながら使用することが多いです。\
基本的にはパラメータあたりのbit数を減らすことでそれに比例して必要なメモリが少なくなります。

また、これに加えて`llama.cpp`などのCPUでモデルを動かせるフレームワークも進化しており、GPUのVRAMではなく、RAMが十分にあれば実用的なレベルで動かすこともできるようになっています。
